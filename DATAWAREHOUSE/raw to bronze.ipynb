{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Landing to bronze NOTEBOOK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import pyspark\n",
    "from delta import *\n",
    "import os\n",
    "import IPython\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - SPARK SESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|name|age|\n",
      "+----+---+\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build Spark session with Delta configurations and Hive support\n",
    "builder = SparkSession.builder.appName(\"MyApp\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.hadoop.io.native.lib.available\", \"true\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hive:hive-exec:2.3.9\")  # Include Hive dependency\n",
    "\n",
    "# Enable Hive support explicitly and get Spark session\n",
    "spark = configure_spark_with_delta_pip(builder.enableHiveSupport()).getOrCreate()\n",
    "\n",
    "# Optionally, you can test by running a Hive query\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS test_hive_table (name STRING, age INT) USING hive\")\n",
    "spark.sql(\"SELECT * FROM test_hive_table\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e:\\DEPIfINALpROJECT\\DATABASEcSV\n",
      "e:\\DEPIfINALpROJECT\\DATABASEcSV\\bronze\n"
     ]
    }
   ],
   "source": [
    "# Set the landing and bronze locations\n",
    "landing_location = os.path.join(os.getcwd())\n",
    "bronze_location = os.path.join(landing_location, \"bronze\")\n",
    "print(landing_location)\n",
    "print(bronze_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of CSV file names\n",
    "file_names = [\"Location.csv\", \"Payment.csv\", \"PaymentMethod.csv\", \"PaymentStatus.csv\",\n",
    "              \"Request.csv\", \"Trip.csv\", \"User.csv\", \"Vehicles.csv\", \"VehicleMakes.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load CSV files into DataFrames\n",
    "def load_csv_files(file_names, landing_location):\n",
    "    \"\"\"\n",
    "    This function takes a list of CSV file names and loads each CSV into a Spark DataFrame.\n",
    "\n",
    "    Args:\n",
    "        file_names (list): List of CSV filenames to be loaded.\n",
    "        landing_location (str): Path where the CSV files are located.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing DataFrames where the key is the file name and the value is the DataFrame.\n",
    "    \"\"\"\n",
    "    # Create an empty dictionary to hold the DataFrames\n",
    "    dataframes = {}\n",
    "    start = 1\n",
    "    # Loop over the filenames and load each CSV into a DataFrame\n",
    "    for file_name in file_names:\n",
    "        # Remove the \".csv\" extension and use it as the DataFrame key\n",
    "        df_name = file_name.split(\".csv\")[0].lower() + \"_df_raw\"\n",
    "        print(df_name)\n",
    "        # Load the CSV into a DataFrame and store it in the dictionary\n",
    "        df_path = f\"{landing_location}/{file_name}\"\n",
    "        dataframes[df_name] = spark.read.format('csv').option(\"header\", \"true\").load(df_path)\n",
    "        \n",
    "    return dataframes\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "location_df_raw\n",
      "payment_df_raw\n",
      "paymentmethod_df_raw\n",
      "paymentstatus_df_raw\n",
      "request_df_raw\n",
      "trip_df_raw\n",
      "user_df_raw\n",
      "vehicles_df_raw\n",
      "vehiclemakes_df_raw\n",
      "dict_items([('location_df_raw', DataFrame[LocationID: string, Longitude: string, Latitude: string]), ('payment_df_raw', DataFrame[PaymentID: string, PaymentMethodID: string, PaymentStatusID: string]), ('paymentmethod_df_raw', DataFrame[PaymentMethodID: string, MethodName: string]), ('paymentstatus_df_raw', DataFrame[PaymentStatusID: string, StatusName: string]), ('request_df_raw', DataFrame[RequestID: string, PassengerID: string, PickupLocationID: string, DropoffLocationID: string, RequestTime: string, AcceptTime: string]), ('trip_df_raw', DataFrame[TripID: string, RequestID: string, DriverID: string, VehicleID: string, PaymentID: string, TripStartTime: string, TripEndTime: string, TripDistance: string, driver_rating: string, BaseFare: string, ExtraFare: string, MtaTax: string, TipAmount: string, TollsAmount: string, ImprovementSurcharge: string]), ('user_df_raw', DataFrame[UserID: string, FullName: string, Email: string, PhoneNumber: string, DriverMeanRating: string]), ('vehicles_df_raw', DataFrame[VehicleID: string, DriverID: string, MakeID: string, Model: string, Year: string, Color: string, LicensePlate: string]), ('vehiclemakes_df_raw', DataFrame[MakeID: string, MakeName: string])])\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "# Load the CSV files into DataFrames\n",
    "dataframes = load_csv_files(file_names, landing_location)\n",
    "\n",
    "# Loop over the dataframes dictionary to assign each DataFrame to a global variable\n",
    "for df_name, df in dataframes.items():\n",
    "    globals()[df_name] = df\n",
    "\n",
    "#location_df_raw.show(5)\n",
    "print(dataframes.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing first 5 rows of location_df_raw:\n",
      "+----------+------------+-----------+\n",
      "|LocationID|   Longitude|   Latitude|\n",
      "+----------+------------+-----------+\n",
      "|----------|   ---------|   --------|\n",
      "|     76173|-73.99945068|40.72192383|\n",
      "|    567607|-73.77745056|40.64664841|\n",
      "|    498838|-73.95517731|40.76498795|\n",
      "|    138392|-73.99216461|40.72513962|\n",
      "+----------+------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- LocationID: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      "\n",
      "Showing first 5 rows of payment_df_raw:\n",
      "+---------+---------------+---------------+\n",
      "|PaymentID|PaymentMethodID|PaymentStatusID|\n",
      "+---------+---------------+---------------+\n",
      "|---------|---------------|---------------|\n",
      "|        6|              6|              4|\n",
      "|        6|              6|              2|\n",
      "|        3|              3|              1|\n",
      "|        6|              6|              4|\n",
      "+---------+---------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- PaymentID: string (nullable = true)\n",
      " |-- PaymentMethodID: string (nullable = true)\n",
      " |-- PaymentStatusID: string (nullable = true)\n",
      "\n",
      "Showing first 5 rows of paymentmethod_df_raw:\n",
      "+---------------+-----------+\n",
      "|PaymentMethodID| MethodName|\n",
      "+---------------+-----------+\n",
      "|---------------| ----------|\n",
      "|              5| Google Pay|\n",
      "|              1|  Apple Pay|\n",
      "|              6|     PayPal|\n",
      "|              3|Credit Card|\n",
      "+---------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- PaymentMethodID: string (nullable = true)\n",
      " |-- MethodName: string (nullable = true)\n",
      "\n",
      "Showing first 5 rows of paymentstatus_df_raw:\n",
      "+---------------+----------+\n",
      "|PaymentStatusID|StatusName|\n",
      "+---------------+----------+\n",
      "|---------------|----------|\n",
      "|              2|    Failed|\n",
      "|              3|   Pending|\n",
      "|              1| Completed|\n",
      "|              4|  Refunded|\n",
      "+---------------+----------+\n",
      "\n",
      "root\n",
      " |-- PaymentStatusID: string (nullable = true)\n",
      " |-- StatusName: string (nullable = true)\n",
      "\n",
      "Showing first 5 rows of request_df_raw:\n",
      "+---------+-----------+----------------+-----------------+--------------------+--------------------+\n",
      "|RequestID|PassengerID|PickupLocationID|DropoffLocationID|         RequestTime|          AcceptTime|\n",
      "+---------+-----------+----------------+-----------------+--------------------+--------------------+\n",
      "|---------|-----------|----------------|-----------------|         -----------|          ----------|\n",
      "|   488700|     244859|           66445|           204936|2015-01-17 21:05:...|2015-01-17 21:07:...|\n",
      "|   557667|     278993|          307727|           185402|2015-01-31 18:45:...|2015-01-31 18:55:...|\n",
      "|   219890|     107381|          315889|            59602|2015-01-08 19:44:...|2015-01-08 19:50:...|\n",
      "|   268467|     133170|          313708|           234206|2015-01-28 07:22:...|2015-01-28 07:24:...|\n",
      "+---------+-----------+----------------+-----------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- RequestID: string (nullable = true)\n",
      " |-- PassengerID: string (nullable = true)\n",
      " |-- PickupLocationID: string (nullable = true)\n",
      " |-- DropoffLocationID: string (nullable = true)\n",
      " |-- RequestTime: string (nullable = true)\n",
      " |-- AcceptTime: string (nullable = true)\n",
      "\n",
      "Showing first 5 rows of trip_df_raw:\n",
      "+------+---------+--------+---------+---------+--------------------+--------------------+------------+-------------+--------+---------+------+---------+-----------+--------------------+\n",
      "|TripID|RequestID|DriverID|VehicleID|PaymentID|       TripStartTime|         TripEndTime|TripDistance|driver_rating|BaseFare|ExtraFare|MtaTax|TipAmount|TollsAmount|ImprovementSurcharge|\n",
      "+------+---------+--------+---------+---------+--------------------+--------------------+------------+-------------+--------+---------+------+---------+-----------+--------------------+\n",
      "|------|---------|--------|---------|---------|       -------------|         -----------|------------|-------------|--------|---------|------|---------|-----------|--------------------|\n",
      "|     1|   369036|  968141|   363309|        6|2015-01-15 19:05:...|2015-01-15 19:23:...|        1.59|         NULL|   12.00|     1.00|   .50|     3.25|        .00|                 .30|\n",
      "|     2|   369031| 1024488|   336402|        6|2015-01-10 20:33:...|2015-01-10 20:53:...|        3.30|         NULL|   14.50|      .50|   .50|     2.00|        .00|                 .30|\n",
      "|     3|   369030|  968141|   287846|        3|2015-01-10 20:33:...|2015-01-10 20:43:...|        1.80|         NULL|    9.50|      .50|   .50|      .00|        .00|                 .30|\n",
      "|     4|   289268|  985570|   336402|        6|2015-01-10 20:33:...|2015-01-10 20:35:...|         .50|         NULL|    3.50|      .50|   .50|      .00|        .00|                 .30|\n",
      "+------+---------+--------+---------+---------+--------------------+--------------------+------------+-------------+--------+---------+------+---------+-----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- TripID: string (nullable = true)\n",
      " |-- RequestID: string (nullable = true)\n",
      " |-- DriverID: string (nullable = true)\n",
      " |-- VehicleID: string (nullable = true)\n",
      " |-- PaymentID: string (nullable = true)\n",
      " |-- TripStartTime: string (nullable = true)\n",
      " |-- TripEndTime: string (nullable = true)\n",
      " |-- TripDistance: string (nullable = true)\n",
      " |-- driver_rating: string (nullable = true)\n",
      " |-- BaseFare: string (nullable = true)\n",
      " |-- ExtraFare: string (nullable = true)\n",
      " |-- MtaTax: string (nullable = true)\n",
      " |-- TipAmount: string (nullable = true)\n",
      " |-- TollsAmount: string (nullable = true)\n",
      " |-- ImprovementSurcharge: string (nullable = true)\n",
      "\n",
      "Showing first 5 rows of user_df_raw:\n",
      "+------+------------+--------------------+--------------+----------------+\n",
      "|UserID|    FullName|               Email|   PhoneNumber|DriverMeanRating|\n",
      "+------+------------+--------------------+--------------+----------------+\n",
      "|------|    --------|               -----|   -----------|----------------|\n",
      "|     1|Aaron Acosta| vbishop@example.net|(677) 367-9557|            NULL|\n",
      "|     2| Aaron Adams| james82@example.com|(332) 224-7965|            NULL|\n",
      "|     3| Aaron Adams|nicholas92@exampl...|(351) 943-8670|            NULL|\n",
      "|     4| Aaron Adams|zjohnson@example.com|(324) 384-5822|            NULL|\n",
      "+------+------------+--------------------+--------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- UserID: string (nullable = true)\n",
      " |-- FullName: string (nullable = true)\n",
      " |-- Email: string (nullable = true)\n",
      " |-- PhoneNumber: string (nullable = true)\n",
      " |-- DriverMeanRating: string (nullable = true)\n",
      "\n",
      "Showing first 5 rows of vehicles_df_raw:\n",
      "+---------+--------+------+--------+----+-----+------------+\n",
      "|VehicleID|DriverID|MakeID|   Model|Year|Color|LicensePlate|\n",
      "+---------+--------+------+--------+----+-----+------------+\n",
      "|---------|--------|------|   -----|----|-----|------------|\n",
      "|        1|  879366|     1|3 Series|1990|Black|       056TZ|\n",
      "|        2|  757688|     1|3 Series|1990|Black|     1HA Y78|\n",
      "|        2|  873185|     1|3 Series|1990|Black|     1HA Y78|\n",
      "|        2|  901239|     1|3 Series|1990|Black|     1HA Y78|\n",
      "+---------+--------+------+--------+----+-----+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- VehicleID: string (nullable = true)\n",
      " |-- DriverID: string (nullable = true)\n",
      " |-- MakeID: string (nullable = true)\n",
      " |-- Model: string (nullable = true)\n",
      " |-- Year: string (nullable = true)\n",
      " |-- Color: string (nullable = true)\n",
      " |-- LicensePlate: string (nullable = true)\n",
      "\n",
      "Showing first 5 rows of vehiclemakes_df_raw:\n",
      "+------+----------+\n",
      "|MakeID|  MakeName|\n",
      "+------+----------+\n",
      "|------|  --------|\n",
      "|     8|    Nissan|\n",
      "|     3|      Ford|\n",
      "|    10|Volkswagen|\n",
      "|     5|   Hyundai|\n",
      "+------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- MakeID: string (nullable = true)\n",
      " |-- MakeName: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loop through each DataFrame in the dictionary and display its content\n",
    "for df_name, df in dataframes.items():\n",
    "    print(f\"Showing first 5 rows of {df_name}:\")\n",
    "    df.show(5)\n",
    "    df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_date_columns(dataframes):\n",
    "    \"\"\"\n",
    "    This function takes a dictionary of DataFrames and adds 'processing_date' and 'modification_date' columns\n",
    "    to each DataFrame.\n",
    "\n",
    "    Args:\n",
    "        dataframes (dict): A dictionary containing DataFrames.\n",
    "    \"\"\"\n",
    "    processing_date = date_trunc('second', current_timestamp())\n",
    "\n",
    "    for df_name, df in dataframes.items():\n",
    "        # Add 'processing_date' and 'modification_date' columns\n",
    "        #df = df.withcolumn(\"_pipeline_run_id\", lit(dbutils.widgets.get('_pipeline_run_id')))\n",
    "        df = df.withColumn(\"_processing_date\", processing_date) \\\n",
    "                .withColumn(\"_input_filename\", input_file_name()) \\\n",
    "                .withColumn(\"_input_file_modification_date\", col(\"_metadata.file_modification_time\"))\n",
    "        \n",
    "        # Update the DataFrame in the dictionary\n",
    "        dataframes[df_name] = df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----------+-------------------+--------------------+-----------------------------+\n",
      "|LocationID|   Longitude|   Latitude|   _processing_date|     _input_filename|_input_file_modification_date|\n",
      "+----------+------------+-----------+-------------------+--------------------+-----------------------------+\n",
      "|----------|   ---------|   --------|2024-10-13 22:29:56|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|     76173|-73.99945068|40.72192383|2024-10-13 22:29:56|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|    567607|-73.77745056|40.64664841|2024-10-13 22:29:56|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|    498838|-73.95517731|40.76498795|2024-10-13 22:29:56|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|    138392|-73.99216461|40.72513962|2024-10-13 22:29:56|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "+----------+------------+-----------+-------------------+--------------------+-----------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Call the function to add date columns to all DataFrames\n",
    "add_date_columns(dataframes)\n",
    "# Loop over the dataframes dictionary to assign each DataFrame to a global variable\n",
    "for df_name, df in dataframes.items():\n",
    "    globals()[df_name] = df\n",
    "\n",
    "location_df_raw.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated DataFrame: location_df_raw\n",
      "+----------+------------+-----------+-------------------+--------------------+-----------------------------+\n",
      "|LocationID|   Longitude|   Latitude|   _processing_date|     _input_filename|_input_file_modification_date|\n",
      "+----------+------------+-----------+-------------------+--------------------+-----------------------------+\n",
      "|----------|   ---------|   --------|2024-10-13 22:29:56|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|     76173|-73.99945068|40.72192383|2024-10-13 22:29:56|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|    567607|-73.77745056|40.64664841|2024-10-13 22:29:56|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|    498838|-73.95517731|40.76498795|2024-10-13 22:29:56|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|    138392|-73.99216461|40.72513962|2024-10-13 22:29:56|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "+----------+------------+-----------+-------------------+--------------------+-----------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Updated DataFrame: payment_df_raw\n",
      "+---------+---------------+---------------+-------------------+--------------------+-----------------------------+\n",
      "|PaymentID|PaymentMethodID|PaymentStatusID|   _processing_date|     _input_filename|_input_file_modification_date|\n",
      "+---------+---------------+---------------+-------------------+--------------------+-----------------------------+\n",
      "|---------|---------------|---------------|2024-10-13 22:29:56|file:///e:/DEPIfI...|         2024-10-08 09:47:...|\n",
      "|        6|              6|              4|2024-10-13 22:29:56|file:///e:/DEPIfI...|         2024-10-08 09:47:...|\n",
      "|        6|              6|              2|2024-10-13 22:29:56|file:///e:/DEPIfI...|         2024-10-08 09:47:...|\n",
      "|        3|              3|              1|2024-10-13 22:29:56|file:///e:/DEPIfI...|         2024-10-08 09:47:...|\n",
      "|        6|              6|              4|2024-10-13 22:29:56|file:///e:/DEPIfI...|         2024-10-08 09:47:...|\n",
      "+---------+---------------+---------------+-------------------+--------------------+-----------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Updated DataFrame: paymentmethod_df_raw\n",
      "+---------------+-----------+-------------------+--------------------+-----------------------------+\n",
      "|PaymentMethodID| MethodName|   _processing_date|     _input_filename|_input_file_modification_date|\n",
      "+---------------+-----------+-------------------+--------------------+-----------------------------+\n",
      "|---------------| ----------|2024-10-13 22:29:57|file:///e:/DEPIfI...|         2024-10-08 09:47:...|\n",
      "|              5| Google Pay|2024-10-13 22:29:57|file:///e:/DEPIfI...|         2024-10-08 09:47:...|\n",
      "|              1|  Apple Pay|2024-10-13 22:29:57|file:///e:/DEPIfI...|         2024-10-08 09:47:...|\n",
      "|              6|     PayPal|2024-10-13 22:29:57|file:///e:/DEPIfI...|         2024-10-08 09:47:...|\n",
      "|              3|Credit Card|2024-10-13 22:29:57|file:///e:/DEPIfI...|         2024-10-08 09:47:...|\n",
      "+---------------+-----------+-------------------+--------------------+-----------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Updated DataFrame: paymentstatus_df_raw\n",
      "+---------------+----------+-------------------+--------------------+-----------------------------+\n",
      "|PaymentStatusID|StatusName|   _processing_date|     _input_filename|_input_file_modification_date|\n",
      "+---------------+----------+-------------------+--------------------+-----------------------------+\n",
      "|---------------|----------|2024-10-13 22:29:57|file:///e:/DEPIfI...|         2024-10-08 09:47:...|\n",
      "|              2|    Failed|2024-10-13 22:29:57|file:///e:/DEPIfI...|         2024-10-08 09:47:...|\n",
      "|              3|   Pending|2024-10-13 22:29:57|file:///e:/DEPIfI...|         2024-10-08 09:47:...|\n",
      "|              1| Completed|2024-10-13 22:29:57|file:///e:/DEPIfI...|         2024-10-08 09:47:...|\n",
      "|              4|  Refunded|2024-10-13 22:29:57|file:///e:/DEPIfI...|         2024-10-08 09:47:...|\n",
      "+---------------+----------+-------------------+--------------------+-----------------------------+\n",
      "\n",
      "Updated DataFrame: request_df_raw\n",
      "+---------+-----------+----------------+-----------------+--------------------+--------------------+-------------------+--------------------+-----------------------------+\n",
      "|RequestID|PassengerID|PickupLocationID|DropoffLocationID|         RequestTime|          AcceptTime|   _processing_date|     _input_filename|_input_file_modification_date|\n",
      "+---------+-----------+----------------+-----------------+--------------------+--------------------+-------------------+--------------------+-----------------------------+\n",
      "|---------|-----------|----------------|-----------------|         -----------|          ----------|2024-10-13 22:29:57|file:///e:/DEPIfI...|         2024-10-08 09:47:...|\n",
      "|   488700|     244859|           66445|           204936|2015-01-17 21:05:...|2015-01-17 21:07:...|2024-10-13 22:29:57|file:///e:/DEPIfI...|         2024-10-08 09:47:...|\n",
      "|   557667|     278993|          307727|           185402|2015-01-31 18:45:...|2015-01-31 18:55:...|2024-10-13 22:29:57|file:///e:/DEPIfI...|         2024-10-08 09:47:...|\n",
      "|   219890|     107381|          315889|            59602|2015-01-08 19:44:...|2015-01-08 19:50:...|2024-10-13 22:29:57|file:///e:/DEPIfI...|         2024-10-08 09:47:...|\n",
      "|   268467|     133170|          313708|           234206|2015-01-28 07:22:...|2015-01-28 07:24:...|2024-10-13 22:29:57|file:///e:/DEPIfI...|         2024-10-08 09:47:...|\n",
      "+---------+-----------+----------------+-----------------+--------------------+--------------------+-------------------+--------------------+-----------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Updated DataFrame: trip_df_raw\n",
      "+------+---------+--------+---------+---------+--------------------+--------------------+------------+-------------+--------+---------+------+---------+-----------+--------------------+-------------------+--------------------+-----------------------------+\n",
      "|TripID|RequestID|DriverID|VehicleID|PaymentID|       TripStartTime|         TripEndTime|TripDistance|driver_rating|BaseFare|ExtraFare|MtaTax|TipAmount|TollsAmount|ImprovementSurcharge|   _processing_date|     _input_filename|_input_file_modification_date|\n",
      "+------+---------+--------+---------+---------+--------------------+--------------------+------------+-------------+--------+---------+------+---------+-----------+--------------------+-------------------+--------------------+-----------------------------+\n",
      "|------|---------|--------|---------|---------|       -------------|         -----------|------------|-------------|--------|---------|------|---------|-----------|--------------------|2024-10-13 22:29:57|file:///e:/DEPIfI...|         2024-10-08 09:47:...|\n",
      "|     1|   369036|  968141|   363309|        6|2015-01-15 19:05:...|2015-01-15 19:23:...|        1.59|         NULL|   12.00|     1.00|   .50|     3.25|        .00|                 .30|2024-10-13 22:29:57|file:///e:/DEPIfI...|         2024-10-08 09:47:...|\n",
      "|     2|   369031| 1024488|   336402|        6|2015-01-10 20:33:...|2015-01-10 20:53:...|        3.30|         NULL|   14.50|      .50|   .50|     2.00|        .00|                 .30|2024-10-13 22:29:57|file:///e:/DEPIfI...|         2024-10-08 09:47:...|\n",
      "|     3|   369030|  968141|   287846|        3|2015-01-10 20:33:...|2015-01-10 20:43:...|        1.80|         NULL|    9.50|      .50|   .50|      .00|        .00|                 .30|2024-10-13 22:29:57|file:///e:/DEPIfI...|         2024-10-08 09:47:...|\n",
      "|     4|   289268|  985570|   336402|        6|2015-01-10 20:33:...|2015-01-10 20:35:...|         .50|         NULL|    3.50|      .50|   .50|      .00|        .00|                 .30|2024-10-13 22:29:57|file:///e:/DEPIfI...|         2024-10-08 09:47:...|\n",
      "+------+---------+--------+---------+---------+--------------------+--------------------+------------+-------------+--------+---------+------+---------+-----------+--------------------+-------------------+--------------------+-----------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Updated DataFrame: user_df_raw\n",
      "+------+------------+--------------------+--------------+----------------+-------------------+--------------------+-----------------------------+\n",
      "|UserID|    FullName|               Email|   PhoneNumber|DriverMeanRating|   _processing_date|     _input_filename|_input_file_modification_date|\n",
      "+------+------------+--------------------+--------------+----------------+-------------------+--------------------+-----------------------------+\n",
      "|------|    --------|               -----|   -----------|----------------|2024-10-13 22:29:57|file:///e:/DEPIfI...|         2024-10-13 13:34:...|\n",
      "|     1|Aaron Acosta| vbishop@example.net|(677) 367-9557|            NULL|2024-10-13 22:29:57|file:///e:/DEPIfI...|         2024-10-13 13:34:...|\n",
      "|     2| Aaron Adams| james82@example.com|(332) 224-7965|            NULL|2024-10-13 22:29:57|file:///e:/DEPIfI...|         2024-10-13 13:34:...|\n",
      "|     3| Aaron Adams|nicholas92@exampl...|(351) 943-8670|            NULL|2024-10-13 22:29:57|file:///e:/DEPIfI...|         2024-10-13 13:34:...|\n",
      "|     4| Aaron Adams|zjohnson@example.com|(324) 384-5822|            NULL|2024-10-13 22:29:57|file:///e:/DEPIfI...|         2024-10-13 13:34:...|\n",
      "+------+------------+--------------------+--------------+----------------+-------------------+--------------------+-----------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Updated DataFrame: vehicles_df_raw\n",
      "+---------+--------+------+--------+----+-----+------------+-------------------+--------------------+-----------------------------+\n",
      "|VehicleID|DriverID|MakeID|   Model|Year|Color|LicensePlate|   _processing_date|     _input_filename|_input_file_modification_date|\n",
      "+---------+--------+------+--------+----+-----+------------+-------------------+--------------------+-----------------------------+\n",
      "|---------|--------|------|   -----|----|-----|------------|2024-10-13 22:29:57|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|        1|  879366|     1|3 Series|1990|Black|       056TZ|2024-10-13 22:29:57|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|        2|  757688|     1|3 Series|1990|Black|     1HA Y78|2024-10-13 22:29:57|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|        2|  873185|     1|3 Series|1990|Black|     1HA Y78|2024-10-13 22:29:57|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|        2|  901239|     1|3 Series|1990|Black|     1HA Y78|2024-10-13 22:29:57|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "+---------+--------+------+--------+----+-----+------------+-------------------+--------------------+-----------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Updated DataFrame: vehiclemakes_df_raw\n",
      "+------+----------+-------------------+--------------------+-----------------------------+\n",
      "|MakeID|  MakeName|   _processing_date|     _input_filename|_input_file_modification_date|\n",
      "+------+----------+-------------------+--------------------+-----------------------------+\n",
      "|------|  --------|2024-10-13 22:29:57|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|     8|    Nissan|2024-10-13 22:29:57|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|     3|      Ford|2024-10-13 22:29:57|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|    10|Volkswagen|2024-10-13 22:29:57|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|     5|   Hyundai|2024-10-13 22:29:57|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "+------+----------+-------------------+--------------------+-----------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Display the updated DataFrames to verify the new columns\n",
    "for df_name, df in dataframes.items():\n",
    "    print(f\"Updated DataFrame: {df_name}\")\n",
    "    df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "def write_to_delta(dataframes, bronze_location):\n",
    "    \"\"\"\n",
    "    This function checks if each table exists in Delta format at the specified location. \n",
    "    If the table exists, it appends the data; otherwise, it creates a new Delta table.\n",
    "\n",
    "    Args:\n",
    "        dataframes (dict): Dictionary of DataFrames.\n",
    "        bronze_location (str): The base path where Delta tables will be stored.\n",
    "    \"\"\"\n",
    "    for df_name, df in dataframes.items():\n",
    "        delta_table_path = f\"{bronze_location}\\\\{df_name}\"\n",
    "        # Check if the Delta table exists\n",
    "        if DeltaTable.isDeltaTable(spark, delta_table_path):\n",
    "            print(f\"Table {df_name} already exists, appending data.\")\n",
    "            df.write.mode(\"append\").format(\"delta\").save(delta_table_path)\n",
    "        else:\n",
    "            print(f\"Creating new table for {df_name}.\")\n",
    "            df.write.mode(\"overwrite\").format(\"delta\").save(delta_table_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new table for location_df_raw.\n",
      "Creating new table for payment_df_raw.\n",
      "Creating new table for paymentmethod_df_raw.\n",
      "Creating new table for paymentstatus_df_raw.\n",
      "Creating new table for request_df_raw.\n",
      "Creating new table for trip_df_raw.\n",
      "Creating new table for user_df_raw.\n",
      "Creating new table for vehicles_df_raw.\n",
      "Creating new table for vehiclemakes_df_raw.\n"
     ]
    }
   ],
   "source": [
    "write_to_delta(dataframes, bronze_location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating external table: bronze.location\n",
      "bronze.location\n",
      "Creating external table: bronze.payment\n",
      "bronze.payment\n",
      "Creating external table: bronze.paymentmethod\n",
      "bronze.paymentmethod\n",
      "Creating external table: bronze.paymentstatus\n",
      "bronze.paymentstatus\n",
      "Creating external table: bronze.request\n",
      "bronze.request\n",
      "Creating external table: bronze.trip\n",
      "bronze.trip\n",
      "Creating external table: bronze.user\n",
      "bronze.user\n",
      "Creating external table: bronze.vehicles\n",
      "bronze.vehicles\n",
      "Creating external table: bronze.vehiclemakes\n",
      "bronze.vehiclemakes\n"
     ]
    }
   ],
   "source": [
    "# After your write_to_delta function\n",
    "for file in file_names:\n",
    "    \n",
    "    table_name = f\"bronze.{file.split('.csv')[0].lower()}\"\n",
    "    print(f\"Creating external table: {table_name}\")\n",
    "    print(table_name.split()[-1])\n",
    "    # Creating schema and external table in Spark SQL\n",
    "    spark.sql(\"CREATE SCHEMA IF NOT EXISTS bronze\")\n",
    "    # Set the current database to 'bronze'\n",
    "    spark.sql(\"USE bronze\")\n",
    "    delta_table_path = f\"{bronze_location}\\\\{table_name.split('.')[-1]}_df_raw\"\n",
    "    spark.sql(f\"CREATE EXTERNAL TABLE IF NOT EXISTS {table_name} USING DELTA LOCATION r'{delta_table_path}'\")\n",
    "    #spark.sql(f\"SELECT * FROM location_df_raw\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+-----------+\n",
      "|namespace|      tableName|isTemporary|\n",
      "+---------+---------------+-----------+\n",
      "|   bronze|       location|      false|\n",
      "|   bronze|        payment|      false|\n",
      "|   bronze| payment_method|      false|\n",
      "|   bronze| payment_status|      false|\n",
      "|   bronze|  paymentmethod|      false|\n",
      "|   bronze|  paymentstatus|      false|\n",
      "|   bronze|        request|      false|\n",
      "|   bronze|test_hive_table|      false|\n",
      "|   bronze|           trip|      false|\n",
      "|   bronze|           user|      false|\n",
      "|   bronze|        vehicle|      false|\n",
      "|   bronze|   vehicle_make|      false|\n",
      "|   bronze|   vehiclemakes|      false|\n",
      "|   bronze|       vehicles|      false|\n",
      "+---------+---------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show all tables in the 'bronze' schema\n",
    "spark.sql(\"SHOW TABLES IN bronze\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Schema:\n",
      "+------------------+\n",
      "|current_database()|\n",
      "+------------------+\n",
      "|            bronze|\n",
      "+------------------+\n",
      "\n",
      "Tables in bronze schema:\n",
      "+---------+---------------+-----------+\n",
      "|namespace|      tableName|isTemporary|\n",
      "+---------+---------------+-----------+\n",
      "|   bronze|       location|      false|\n",
      "|   bronze|        payment|      false|\n",
      "|   bronze| payment_method|      false|\n",
      "|   bronze| payment_status|      false|\n",
      "|   bronze|  paymentmethod|      false|\n",
      "|   bronze|  paymentstatus|      false|\n",
      "|   bronze|        request|      false|\n",
      "|   bronze|test_hive_table|      false|\n",
      "|   bronze|           trip|      false|\n",
      "|   bronze|           user|      false|\n",
      "|   bronze|        vehicle|      false|\n",
      "|   bronze|   vehicle_make|      false|\n",
      "|   bronze|   vehiclemakes|      false|\n",
      "|   bronze|       vehicles|      false|\n",
      "+---------+---------------+-----------+\n",
      "\n",
      "+----------+---------+--------+-------------------+--------------------+-----------------------------+\n",
      "|LocationID|Longitude|Latitude|   _processing_date|     _input_filename|_input_file_modification_date|\n",
      "+----------+---------+--------+-------------------+--------------------+-----------------------------+\n",
      "|----------|---------|--------|2024-10-13 22:29:58|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "+----------+---------+--------+-------------------+--------------------+-----------------------------+\n",
      "\n",
      "+---------+---------------+---------------+-------------------+--------------------+-----------------------------+\n",
      "|PaymentID|PaymentMethodID|PaymentStatusID|   _processing_date|     _input_filename|_input_file_modification_date|\n",
      "+---------+---------------+---------------+-------------------+--------------------+-----------------------------+\n",
      "|---------|---------------|---------------|2024-10-13 22:30:05|file:///e:/DEPIfI...|         2024-10-08 09:47:...|\n",
      "+---------+---------------+---------------+-------------------+--------------------+-----------------------------+\n",
      "\n",
      "Describing history for bronze.location:\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Verify the current schema\n",
    "print(\"Current Schema:\")\n",
    "spark.sql(\"SELECT current_database()\").show()\n",
    "\n",
    "# List all tables in the bronze schema\n",
    "print(\"Tables in bronze schema:\")\n",
    "spark.sql(\"SHOW TABLES IN bronze\").show()\n",
    "spark.sql(\"SELECT * FROM bronze.location LIMIT 1\").show()\n",
    "spark.sql(\"SELECT * FROM bronze.payment LIMIT 1\").show()\n",
    "\n",
    "\n",
    "# Describe the history of the Delta table\n",
    "print(\"Describing history for bronze.location:\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+----------------------------------------------------------------------+------------+-----------------------------------+\n",
      "|version|timestamp              |userId|userName|operation|operationParameters                   |job |notebook|clusterId|readVersion|isolationLevel|isBlindAppend|operationMetrics                                                      |userMetadata|engineInfo                         |\n",
      "+-------+-----------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+----------------------------------------------------------------------+------------+-----------------------------------+\n",
      "|0      |2024-10-13 22:30:04.049|NULL  |NULL    |WRITE    |{mode -> Overwrite, partitionBy -> []}|NULL|NULL    |NULL     |NULL       |Serializable  |false        |{numFiles -> 10, numOutputRows -> 1148685, numOutputBytes -> 14604261}|NULL        |Apache-Spark/3.5.3 Delta-Lake/3.2.0|\n",
      "+-------+-----------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+----------------------------------------------------------------------+------------+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from delta.tables import *\n",
    "\n",
    "deltaTable = DeltaTable.forPath(spark, \"bronze\\\\location_df_raw\")\n",
    "fullHistoryDF = deltaTable.history()\n",
    "fullHistoryDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+\n",
      "|   _processing_date|    cnt|\n",
      "+-------------------+-------+\n",
      "|2024-10-13 22:29:58|1148685|\n",
      "+-------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT _processing_date, count(*) cnt FROM bronze.location GROUP BY _processing_date\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----------+-------------------+--------------------+-----------------------------+\n",
      "|LocationID|   Longitude|   Latitude|   _processing_date|     _input_filename|_input_file_modification_date|\n",
      "+----------+------------+-----------+-------------------+--------------------+-----------------------------+\n",
      "|----------|   ---------|   --------|2024-10-13 22:29:58|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|     76173|-73.99945068|40.72192383|2024-10-13 22:29:58|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|    567607|-73.77745056|40.64664841|2024-10-13 22:29:58|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|    498838|-73.95517731|40.76498795|2024-10-13 22:29:58|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|    138392|-73.99216461|40.72513962|2024-10-13 22:29:58|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|    532266|-73.94660187|40.77573395|2024-10-13 22:29:58|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|    144884|-73.99170685|40.76995850|2024-10-13 22:29:58|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|     60322|-74.00185394|40.71962357|2024-10-13 22:29:58|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|    183905|-73.98905182|40.75791168|2024-10-13 22:29:58|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|    474267|-73.95960236|40.77397156|2024-10-13 22:29:58|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|     48725|-74.00360870|40.72516632|2024-10-13 22:29:58|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|    131712|-73.99272919|40.76867676|2024-10-13 22:29:58|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|    131679|-73.99272919|40.73368835|2024-10-13 22:29:58|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|    528027|-73.94848633|40.78642273|2024-10-13 22:29:58|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|    199157|-73.98799896|40.73763657|2024-10-13 22:29:58|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|    173479|-73.98983002|40.74574661|2024-10-13 22:29:58|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|    127005|-73.99319458|40.76365662|2024-10-13 22:29:58|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|    249017|-73.98426056|40.77954483|2024-10-13 22:29:58|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|    278182|-73.98220062|40.76637650|2024-10-13 22:29:58|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|    507129|-73.95393372|40.76709366|2024-10-13 22:29:58|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "+----------+------------+-----------+-------------------+--------------------+-----------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from bronze.location\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = spark.read.table(\"bronze.user\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "location\n",
      "bronze.location\n",
      "payment\n",
      "bronze.payment\n",
      "paymentmethod\n",
      "bronze.paymentmethod\n",
      "paymentstatus\n",
      "bronze.paymentstatus\n",
      "request\n",
      "bronze.request\n",
      "trip\n",
      "bronze.trip\n",
      "user\n",
      "bronze.user\n",
      "vehicles\n",
      "bronze.vehicles\n",
      "vehiclemakes\n",
      "bronze.vehiclemakes\n"
     ]
    }
   ],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Dictionary to store the DeltaTable objects\n",
    "delta_tables = {}\n",
    "\n",
    "for file in file_names:\n",
    "    file_name = file.split('.csv')[0].lower()  # Extract the file name (without .csv extension)\n",
    "    table_name = f\"bronze.{file_name}\"  # Define the table name in the bronze schema\n",
    "    \n",
    "    print(file_name)\n",
    "    print(table_name)\n",
    "\n",
    "    # Store the DeltaTable object in the dictionary using the file name as the key\n",
    "    delta_tables[file_name] = DeltaTable.forName(spark, table_name)\n",
    "\n",
    "    # Vacuum the table if it has not been vacuumed in the last 30 days.\n",
    "    if delta_tables[file_name].history(30).filter(\"operation = 'VACUUM START'\").count() == 0:\n",
    "        delta_tables[file_name].optimize()\n",
    "        delta_tables[file_name].vacuum()  # Default = 7 days\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "| 1148685|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT COUNT(*) FROM bronze.location\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spark.stop()\n",
    "%reset -f\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
