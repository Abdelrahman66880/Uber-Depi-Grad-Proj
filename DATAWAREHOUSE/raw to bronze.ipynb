{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import pyspark\n",
    "from delta import *\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|name|age|\n",
      "+----+---+\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build Spark session with Delta configurations and Hive support\n",
    "builder = SparkSession.builder.appName(\"MyApp\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.hadoop.io.native.lib.available\", \"true\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hive:hive-exec:2.3.9\")  # Include Hive dependency\n",
    "\n",
    "# Enable Hive support explicitly and get Spark session\n",
    "spark = configure_spark_with_delta_pip(builder.enableHiveSupport()).getOrCreate()\n",
    "\n",
    "# Optionally, you can test by running a Hive query\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS test_hive_table (name STRING, age INT) USING hive\")\n",
    "spark.sql(\"SELECT * FROM test_hive_table\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "#spark.conf.set(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file:///C:/Users/mouda/.ivy2/jars/io.delta_delta-spark_2.12-3.2.0.jar,file:///C:/Users/mouda/.ivy2/jars/io.delta_delta-storage-3.2.0.jar,file:///C:/Users/mouda/.ivy2/jars/org.antlr_antlr4-runtime-4.9.3.jar\n"
     ]
    }
   ],
   "source": [
    "print(spark.sparkContext.getConf().get(\"spark.jars\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+-----------+\n",
      "|namespace|      tableName|isTemporary|\n",
      "+---------+---------------+-----------+\n",
      "|  default|test_hive_table|      false|\n",
      "+---------+---------------+-----------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Check if Delta is available\n",
    "print(spark.sql(\"SHOW TABLES\").show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e:\\DEPIfINALpROJECT\\DATABASEcSV\n",
      "e:\\DEPIfINALpROJECT\\DATABASEcSV\\bronze\n"
     ]
    }
   ],
   "source": [
    "# Set the landing and bronze locations\n",
    "landing_location = os.path.join(os.getcwd())\n",
    "bronze_location = os.path.join(landing_location, \"bronze\")\n",
    "print(landing_location)\n",
    "print(bronze_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of CSV file names\n",
    "file_names = [\"Location.csv\", \"Payment.csv\", \"PaymentMethod.csv\", \"PaymentStatus.csv\",\n",
    "              \"Request.csv\", \"Trip.csv\", \"User.csv\", \"Vehicles.csv\", \"VehicleMakes.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load CSV files into DataFrames\n",
    "def load_csv_files(file_names, landing_location):\n",
    "    \"\"\"\n",
    "    This function takes a list of CSV file names and loads each CSV into a Spark DataFrame.\n",
    "\n",
    "    Args:\n",
    "        file_names (list): List of CSV filenames to be loaded.\n",
    "        landing_location (str): Path where the CSV files are located.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing DataFrames where the key is the file name and the value is the DataFrame.\n",
    "    \"\"\"\n",
    "    # Create an empty dictionary to hold the DataFrames\n",
    "    dataframes = {}\n",
    "    start = 1\n",
    "    # Loop over the filenames and load each CSV into a DataFrame\n",
    "    for file_name in file_names:\n",
    "        # Remove the \".csv\" extension and use it as the DataFrame key\n",
    "        df_name = file_name.split(\".csv\")[0].lower() + \"_df_raw\"\n",
    "        print(df_name)\n",
    "        # Load the CSV into a DataFrame and store it in the dictionary\n",
    "        df_path = f\"{landing_location}/{file_name}\"\n",
    "        dataframes[df_name] = spark.read.format('csv').option(\"header\", \"true\").load(df_path)\n",
    "        \n",
    "    return dataframes\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "location_df_raw\n",
      "payment_df_raw\n",
      "paymentmethod_df_raw\n",
      "paymentstatus_df_raw\n",
      "request_df_raw\n",
      "trip_df_raw\n",
      "user_df_raw\n",
      "vehicles_df_raw\n",
      "vehiclemakes_df_raw\n",
      "dict_items([('location_df_raw', DataFrame[LocationID: string, Longitude: string, Latitude: string]), ('payment_df_raw', DataFrame[PaymentID: string, PaymentMethodID: string, PaymentStatusID: string]), ('paymentmethod_df_raw', DataFrame[PaymentMethodID: string, MethodName: string]), ('paymentstatus_df_raw', DataFrame[PaymentStatusID: string, StatusName: string]), ('request_df_raw', DataFrame[RequestID: string, PassengerID: string, PickupLocationID: string, DropoffLocationID: string, RequestTime: string, AcceptTime: string]), ('trip_df_raw', DataFrame[TripID: string, RequestID: string, DriverID: string, VehicleID: string, PaymentID: string, TripStartTime: string, TripEndTime: string, TripDistance: string, driver_rating: string, BaseFare: string, ExtraFare: string, MtaTax: string, TipAmount: string, TollsAmount: string, ImprovementSurcharge: string]), ('user_df_raw', DataFrame[UserID: string, FullName: string, Email: string, PhoneNumber: string, DriverMeanRating: string]), ('vehicles_df_raw', DataFrame[VehicleID: string, DriverID: string, MakeID: string, Model: string, Year: string, Color: string, LicensePlate: string]), ('vehiclemakes_df_raw', DataFrame[MakeID: string, MakeName: string])])\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "# Load the CSV files into DataFrames\n",
    "dataframes = load_csv_files(file_names, landing_location)\n",
    "\n",
    "# Loop over the dataframes dictionary to assign each DataFrame to a global variable\n",
    "for df_name, df in dataframes.items():\n",
    "    globals()[df_name] = df\n",
    "\n",
    "#location_df_raw.show(5)\n",
    "print(dataframes.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing first 5 rows of location_df_raw:\n",
      "+----------+------------+-----------+\n",
      "|LocationID|   Longitude|   Latitude|\n",
      "+----------+------------+-----------+\n",
      "|----------|   ---------|   --------|\n",
      "|     76173|-73.99945068|40.72192383|\n",
      "|    567607|-73.77745056|40.64664841|\n",
      "|    498838|-73.95517731|40.76498795|\n",
      "|    138392|-73.99216461|40.72513962|\n",
      "+----------+------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- LocationID: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      "\n",
      "Showing first 5 rows of payment_df_raw:\n",
      "+---------+---------------+---------------+\n",
      "|PaymentID|PaymentMethodID|PaymentStatusID|\n",
      "+---------+---------------+---------------+\n",
      "|---------|---------------|---------------|\n",
      "|        6|              6|              4|\n",
      "|        6|              6|              2|\n",
      "|        3|              3|              1|\n",
      "|        6|              6|              4|\n",
      "+---------+---------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- PaymentID: string (nullable = true)\n",
      " |-- PaymentMethodID: string (nullable = true)\n",
      " |-- PaymentStatusID: string (nullable = true)\n",
      "\n",
      "Showing first 5 rows of paymentmethod_df_raw:\n",
      "+---------------+-----------+\n",
      "|PaymentMethodID| MethodName|\n",
      "+---------------+-----------+\n",
      "|---------------| ----------|\n",
      "|              5| Google Pay|\n",
      "|              1|  Apple Pay|\n",
      "|              6|     PayPal|\n",
      "|              3|Credit Card|\n",
      "+---------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- PaymentMethodID: string (nullable = true)\n",
      " |-- MethodName: string (nullable = true)\n",
      "\n",
      "Showing first 5 rows of paymentstatus_df_raw:\n",
      "+---------------+----------+\n",
      "|PaymentStatusID|StatusName|\n",
      "+---------------+----------+\n",
      "|---------------|----------|\n",
      "|              2|    Failed|\n",
      "|              3|   Pending|\n",
      "|              1| Completed|\n",
      "|              4|  Refunded|\n",
      "+---------------+----------+\n",
      "\n",
      "root\n",
      " |-- PaymentStatusID: string (nullable = true)\n",
      " |-- StatusName: string (nullable = true)\n",
      "\n",
      "Showing first 5 rows of request_df_raw:\n",
      "+---------+-----------+----------------+-----------------+--------------------+--------------------+\n",
      "|RequestID|PassengerID|PickupLocationID|DropoffLocationID|         RequestTime|          AcceptTime|\n",
      "+---------+-----------+----------------+-----------------+--------------------+--------------------+\n",
      "|---------|-----------|----------------|-----------------|         -----------|          ----------|\n",
      "|   488700|     244859|           66445|           204936|2015-01-17 21:05:...|2015-01-17 21:07:...|\n",
      "|   557667|     278993|          307727|           185402|2015-01-31 18:45:...|2015-01-31 18:55:...|\n",
      "|   219890|     107381|          315889|            59602|2015-01-08 19:44:...|2015-01-08 19:50:...|\n",
      "|   268467|     133170|          313708|           234206|2015-01-28 07:22:...|2015-01-28 07:24:...|\n",
      "+---------+-----------+----------------+-----------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- RequestID: string (nullable = true)\n",
      " |-- PassengerID: string (nullable = true)\n",
      " |-- PickupLocationID: string (nullable = true)\n",
      " |-- DropoffLocationID: string (nullable = true)\n",
      " |-- RequestTime: string (nullable = true)\n",
      " |-- AcceptTime: string (nullable = true)\n",
      "\n",
      "Showing first 5 rows of trip_df_raw:\n",
      "+------+---------+--------+---------+---------+--------------------+--------------------+------------+-------------+--------+---------+------+---------+-----------+--------------------+\n",
      "|TripID|RequestID|DriverID|VehicleID|PaymentID|       TripStartTime|         TripEndTime|TripDistance|driver_rating|BaseFare|ExtraFare|MtaTax|TipAmount|TollsAmount|ImprovementSurcharge|\n",
      "+------+---------+--------+---------+---------+--------------------+--------------------+------------+-------------+--------+---------+------+---------+-----------+--------------------+\n",
      "|------|---------|--------|---------|---------|       -------------|         -----------|------------|-------------|--------|---------|------|---------|-----------|--------------------|\n",
      "|     1|   369036|  968141|   363309|        6|2015-01-15 19:05:...|2015-01-15 19:23:...|        1.59|         NULL|   12.00|     1.00|   .50|     3.25|        .00|                 .30|\n",
      "|     2|   369031| 1024488|   336402|        6|2015-01-10 20:33:...|2015-01-10 20:53:...|        3.30|         NULL|   14.50|      .50|   .50|     2.00|        .00|                 .30|\n",
      "|     3|   369030|  968141|   287846|        3|2015-01-10 20:33:...|2015-01-10 20:43:...|        1.80|         NULL|    9.50|      .50|   .50|      .00|        .00|                 .30|\n",
      "|     4|   289268|  985570|   336402|        6|2015-01-10 20:33:...|2015-01-10 20:35:...|         .50|         NULL|    3.50|      .50|   .50|      .00|        .00|                 .30|\n",
      "+------+---------+--------+---------+---------+--------------------+--------------------+------------+-------------+--------+---------+------+---------+-----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- TripID: string (nullable = true)\n",
      " |-- RequestID: string (nullable = true)\n",
      " |-- DriverID: string (nullable = true)\n",
      " |-- VehicleID: string (nullable = true)\n",
      " |-- PaymentID: string (nullable = true)\n",
      " |-- TripStartTime: string (nullable = true)\n",
      " |-- TripEndTime: string (nullable = true)\n",
      " |-- TripDistance: string (nullable = true)\n",
      " |-- driver_rating: string (nullable = true)\n",
      " |-- BaseFare: string (nullable = true)\n",
      " |-- ExtraFare: string (nullable = true)\n",
      " |-- MtaTax: string (nullable = true)\n",
      " |-- TipAmount: string (nullable = true)\n",
      " |-- TollsAmount: string (nullable = true)\n",
      " |-- ImprovementSurcharge: string (nullable = true)\n",
      "\n",
      "Showing first 5 rows of user_df_raw:\n",
      "+------+------------+--------------------+--------------+----------------+\n",
      "|UserID|    FullName|               Email|   PhoneNumber|DriverMeanRating|\n",
      "+------+------------+--------------------+--------------+----------------+\n",
      "|------|    --------|               -----|   -----------|----------------|\n",
      "|     1|Aaron Acosta| vbishop@example.net|(677) 367-9557|            NULL|\n",
      "|     2| Aaron Adams| james82@example.com|(332) 224-7965|            NULL|\n",
      "|     3| Aaron Adams|nicholas92@exampl...|(351) 943-8670|            NULL|\n",
      "|     4| Aaron Adams|zjohnson@example.com|(324) 384-5822|            NULL|\n",
      "+------+------------+--------------------+--------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- UserID: string (nullable = true)\n",
      " |-- FullName: string (nullable = true)\n",
      " |-- Email: string (nullable = true)\n",
      " |-- PhoneNumber: string (nullable = true)\n",
      " |-- DriverMeanRating: string (nullable = true)\n",
      "\n",
      "Showing first 5 rows of vehicles_df_raw:\n",
      "+---------+--------+------+--------+----+-----+------------+\n",
      "|VehicleID|DriverID|MakeID|   Model|Year|Color|LicensePlate|\n",
      "+---------+--------+------+--------+----+-----+------------+\n",
      "|---------|--------|------|   -----|----|-----|------------|\n",
      "|        1|  879366|     1|3 Series|1990|Black|       056TZ|\n",
      "|        2|  757688|     1|3 Series|1990|Black|     1HA Y78|\n",
      "|        2|  873185|     1|3 Series|1990|Black|     1HA Y78|\n",
      "|        2|  901239|     1|3 Series|1990|Black|     1HA Y78|\n",
      "+---------+--------+------+--------+----+-----+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- VehicleID: string (nullable = true)\n",
      " |-- DriverID: string (nullable = true)\n",
      " |-- MakeID: string (nullable = true)\n",
      " |-- Model: string (nullable = true)\n",
      " |-- Year: string (nullable = true)\n",
      " |-- Color: string (nullable = true)\n",
      " |-- LicensePlate: string (nullable = true)\n",
      "\n",
      "Showing first 5 rows of vehiclemakes_df_raw:\n",
      "+------+----------+\n",
      "|MakeID|  MakeName|\n",
      "+------+----------+\n",
      "|------|  --------|\n",
      "|     8|    Nissan|\n",
      "|     3|      Ford|\n",
      "|    10|Volkswagen|\n",
      "|     5|   Hyundai|\n",
      "+------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- MakeID: string (nullable = true)\n",
      " |-- MakeName: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loop through each DataFrame in the dictionary and display its content\n",
    "for df_name, df in dataframes.items():\n",
    "    print(f\"Showing first 5 rows of {df_name}:\")\n",
    "    df.show(5)\n",
    "    df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_date_columns(dataframes):\n",
    "    \"\"\"\n",
    "    This function takes a dictionary of DataFrames and adds 'processing_date' and 'modification_date' columns\n",
    "    to each DataFrame.\n",
    "\n",
    "    Args:\n",
    "        dataframes (dict): A dictionary containing DataFrames.\n",
    "    \"\"\"\n",
    "    processing_date = date_trunc('second', current_timestamp())\n",
    "\n",
    "    for df_name, df in dataframes.items():\n",
    "        # Add 'processing_date' and 'modification_date' columns\n",
    "        #df = df.withcolumn(\"_pipeline_run_id\", lit(dbutils.widgets.get('_pipeline_run_id')))\n",
    "        df = df.withColumn(\"_processing_date\", processing_date) \\\n",
    "                .withColumn(\"_input_filename\", input_file_name()) \\\n",
    "                .withColumn(\"_input_file_modification_date\", col(\"_metadata.file_modification_time\"))\n",
    "        \n",
    "        # Update the DataFrame in the dictionary\n",
    "        dataframes[df_name] = df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----------+-------------------+--------------------+-----------------------------+\n",
      "|LocationID|   Longitude|   Latitude|   _processing_date|     _input_filename|_input_file_modification_date|\n",
      "+----------+------------+-----------+-------------------+--------------------+-----------------------------+\n",
      "|----------|   ---------|   --------|2024-10-11 08:21:43|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|     76173|-73.99945068|40.72192383|2024-10-11 08:21:43|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|    567607|-73.77745056|40.64664841|2024-10-11 08:21:43|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|    498838|-73.95517731|40.76498795|2024-10-11 08:21:43|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|    138392|-73.99216461|40.72513962|2024-10-11 08:21:43|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "+----------+------------+-----------+-------------------+--------------------+-----------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Call the function to add date columns to all DataFrames\n",
    "add_date_columns(dataframes)\n",
    "# Loop over the dataframes dictionary to assign each DataFrame to a global variable\n",
    "for df_name, df in dataframes.items():\n",
    "    globals()[df_name] = df\n",
    "\n",
    "location_df_raw.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated DataFrame: location_df_raw\n",
      "+----------+------------+-----------+-------------------+--------------------+-----------------------------+\n",
      "|LocationID|   Longitude|   Latitude|   _processing_date|     _input_filename|_input_file_modification_date|\n",
      "+----------+------------+-----------+-------------------+--------------------+-----------------------------+\n",
      "|----------|   ---------|   --------|2024-10-11 08:21:43|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|     76173|-73.99945068|40.72192383|2024-10-11 08:21:43|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|    567607|-73.77745056|40.64664841|2024-10-11 08:21:43|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|    498838|-73.95517731|40.76498795|2024-10-11 08:21:43|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|    138392|-73.99216461|40.72513962|2024-10-11 08:21:43|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "+----------+------------+-----------+-------------------+--------------------+-----------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Updated DataFrame: payment_df_raw\n",
      "+---------+---------------+---------------+-------------------+--------------------+-----------------------------+\n",
      "|PaymentID|PaymentMethodID|PaymentStatusID|   _processing_date|     _input_filename|_input_file_modification_date|\n",
      "+---------+---------------+---------------+-------------------+--------------------+-----------------------------+\n",
      "|---------|---------------|---------------|2024-10-11 08:21:43|file:///e:/DEPIfI...|         2024-10-08 09:47:...|\n",
      "|        6|              6|              4|2024-10-11 08:21:43|file:///e:/DEPIfI...|         2024-10-08 09:47:...|\n",
      "|        6|              6|              2|2024-10-11 08:21:43|file:///e:/DEPIfI...|         2024-10-08 09:47:...|\n",
      "|        3|              3|              1|2024-10-11 08:21:43|file:///e:/DEPIfI...|         2024-10-08 09:47:...|\n",
      "|        6|              6|              4|2024-10-11 08:21:43|file:///e:/DEPIfI...|         2024-10-08 09:47:...|\n",
      "+---------+---------------+---------------+-------------------+--------------------+-----------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Updated DataFrame: paymentmethod_df_raw\n",
      "+---------------+-----------+-------------------+--------------------+-----------------------------+\n",
      "|PaymentMethodID| MethodName|   _processing_date|     _input_filename|_input_file_modification_date|\n",
      "+---------------+-----------+-------------------+--------------------+-----------------------------+\n",
      "|---------------| ----------|2024-10-11 08:21:43|file:///e:/DEPIfI...|         2024-10-08 09:47:...|\n",
      "|              5| Google Pay|2024-10-11 08:21:43|file:///e:/DEPIfI...|         2024-10-08 09:47:...|\n",
      "|              1|  Apple Pay|2024-10-11 08:21:43|file:///e:/DEPIfI...|         2024-10-08 09:47:...|\n",
      "|              6|     PayPal|2024-10-11 08:21:43|file:///e:/DEPIfI...|         2024-10-08 09:47:...|\n",
      "|              3|Credit Card|2024-10-11 08:21:43|file:///e:/DEPIfI...|         2024-10-08 09:47:...|\n",
      "+---------------+-----------+-------------------+--------------------+-----------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Updated DataFrame: paymentstatus_df_raw\n",
      "+---------------+----------+-------------------+--------------------+-----------------------------+\n",
      "|PaymentStatusID|StatusName|   _processing_date|     _input_filename|_input_file_modification_date|\n",
      "+---------------+----------+-------------------+--------------------+-----------------------------+\n",
      "|---------------|----------|2024-10-11 08:21:43|file:///e:/DEPIfI...|         2024-10-08 09:47:...|\n",
      "|              2|    Failed|2024-10-11 08:21:43|file:///e:/DEPIfI...|         2024-10-08 09:47:...|\n",
      "|              3|   Pending|2024-10-11 08:21:43|file:///e:/DEPIfI...|         2024-10-08 09:47:...|\n",
      "|              1| Completed|2024-10-11 08:21:43|file:///e:/DEPIfI...|         2024-10-08 09:47:...|\n",
      "|              4|  Refunded|2024-10-11 08:21:43|file:///e:/DEPIfI...|         2024-10-08 09:47:...|\n",
      "+---------------+----------+-------------------+--------------------+-----------------------------+\n",
      "\n",
      "Updated DataFrame: request_df_raw\n",
      "+---------+-----------+----------------+-----------------+--------------------+--------------------+-------------------+--------------------+-----------------------------+\n",
      "|RequestID|PassengerID|PickupLocationID|DropoffLocationID|         RequestTime|          AcceptTime|   _processing_date|     _input_filename|_input_file_modification_date|\n",
      "+---------+-----------+----------------+-----------------+--------------------+--------------------+-------------------+--------------------+-----------------------------+\n",
      "|---------|-----------|----------------|-----------------|         -----------|          ----------|2024-10-11 08:21:43|file:///e:/DEPIfI...|         2024-10-08 09:47:...|\n",
      "|   488700|     244859|           66445|           204936|2015-01-17 21:05:...|2015-01-17 21:07:...|2024-10-11 08:21:43|file:///e:/DEPIfI...|         2024-10-08 09:47:...|\n",
      "|   557667|     278993|          307727|           185402|2015-01-31 18:45:...|2015-01-31 18:55:...|2024-10-11 08:21:43|file:///e:/DEPIfI...|         2024-10-08 09:47:...|\n",
      "|   219890|     107381|          315889|            59602|2015-01-08 19:44:...|2015-01-08 19:50:...|2024-10-11 08:21:43|file:///e:/DEPIfI...|         2024-10-08 09:47:...|\n",
      "|   268467|     133170|          313708|           234206|2015-01-28 07:22:...|2015-01-28 07:24:...|2024-10-11 08:21:43|file:///e:/DEPIfI...|         2024-10-08 09:47:...|\n",
      "+---------+-----------+----------------+-----------------+--------------------+--------------------+-------------------+--------------------+-----------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Updated DataFrame: trip_df_raw\n",
      "+------+---------+--------+---------+---------+--------------------+--------------------+------------+-------------+--------+---------+------+---------+-----------+--------------------+-------------------+--------------------+-----------------------------+\n",
      "|TripID|RequestID|DriverID|VehicleID|PaymentID|       TripStartTime|         TripEndTime|TripDistance|driver_rating|BaseFare|ExtraFare|MtaTax|TipAmount|TollsAmount|ImprovementSurcharge|   _processing_date|     _input_filename|_input_file_modification_date|\n",
      "+------+---------+--------+---------+---------+--------------------+--------------------+------------+-------------+--------+---------+------+---------+-----------+--------------------+-------------------+--------------------+-----------------------------+\n",
      "|------|---------|--------|---------|---------|       -------------|         -----------|------------|-------------|--------|---------|------|---------|-----------|--------------------|2024-10-11 08:21:43|file:///e:/DEPIfI...|         2024-10-08 09:47:...|\n",
      "|     1|   369036|  968141|   363309|        6|2015-01-15 19:05:...|2015-01-15 19:23:...|        1.59|         NULL|   12.00|     1.00|   .50|     3.25|        .00|                 .30|2024-10-11 08:21:43|file:///e:/DEPIfI...|         2024-10-08 09:47:...|\n",
      "|     2|   369031| 1024488|   336402|        6|2015-01-10 20:33:...|2015-01-10 20:53:...|        3.30|         NULL|   14.50|      .50|   .50|     2.00|        .00|                 .30|2024-10-11 08:21:43|file:///e:/DEPIfI...|         2024-10-08 09:47:...|\n",
      "|     3|   369030|  968141|   287846|        3|2015-01-10 20:33:...|2015-01-10 20:43:...|        1.80|         NULL|    9.50|      .50|   .50|      .00|        .00|                 .30|2024-10-11 08:21:43|file:///e:/DEPIfI...|         2024-10-08 09:47:...|\n",
      "|     4|   289268|  985570|   336402|        6|2015-01-10 20:33:...|2015-01-10 20:35:...|         .50|         NULL|    3.50|      .50|   .50|      .00|        .00|                 .30|2024-10-11 08:21:43|file:///e:/DEPIfI...|         2024-10-08 09:47:...|\n",
      "+------+---------+--------+---------+---------+--------------------+--------------------+------------+-------------+--------+---------+------+---------+-----------+--------------------+-------------------+--------------------+-----------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Updated DataFrame: user_df_raw\n",
      "+------+------------+--------------------+--------------+----------------+-------------------+--------------------+-----------------------------+\n",
      "|UserID|    FullName|               Email|   PhoneNumber|DriverMeanRating|   _processing_date|     _input_filename|_input_file_modification_date|\n",
      "+------+------------+--------------------+--------------+----------------+-------------------+--------------------+-----------------------------+\n",
      "|------|    --------|               -----|   -----------|----------------|2024-10-11 08:21:43|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|     1|Aaron Acosta| vbishop@example.net|(677) 367-9557|            NULL|2024-10-11 08:21:43|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|     2| Aaron Adams| james82@example.com|(332) 224-7965|            NULL|2024-10-11 08:21:43|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|     3| Aaron Adams|nicholas92@exampl...|(351) 943-8670|            NULL|2024-10-11 08:21:43|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|     4| Aaron Adams|zjohnson@example.com|(324) 384-5822|            NULL|2024-10-11 08:21:43|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "+------+------------+--------------------+--------------+----------------+-------------------+--------------------+-----------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Updated DataFrame: vehicles_df_raw\n",
      "+---------+--------+------+--------+----+-----+------------+-------------------+--------------------+-----------------------------+\n",
      "|VehicleID|DriverID|MakeID|   Model|Year|Color|LicensePlate|   _processing_date|     _input_filename|_input_file_modification_date|\n",
      "+---------+--------+------+--------+----+-----+------------+-------------------+--------------------+-----------------------------+\n",
      "|---------|--------|------|   -----|----|-----|------------|2024-10-11 08:21:43|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|        1|  879366|     1|3 Series|1990|Black|       056TZ|2024-10-11 08:21:43|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|        2|  757688|     1|3 Series|1990|Black|     1HA Y78|2024-10-11 08:21:43|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|        2|  873185|     1|3 Series|1990|Black|     1HA Y78|2024-10-11 08:21:43|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|        2|  901239|     1|3 Series|1990|Black|     1HA Y78|2024-10-11 08:21:43|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "+---------+--------+------+--------+----+-----+------------+-------------------+--------------------+-----------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Updated DataFrame: vehiclemakes_df_raw\n",
      "+------+----------+-------------------+--------------------+-----------------------------+\n",
      "|MakeID|  MakeName|   _processing_date|     _input_filename|_input_file_modification_date|\n",
      "+------+----------+-------------------+--------------------+-----------------------------+\n",
      "|------|  --------|2024-10-11 08:21:44|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|     8|    Nissan|2024-10-11 08:21:44|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|     3|      Ford|2024-10-11 08:21:44|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|    10|Volkswagen|2024-10-11 08:21:44|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|     5|   Hyundai|2024-10-11 08:21:44|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "+------+----------+-------------------+--------------------+-----------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Display the updated DataFrames to verify the new columns\n",
    "for df_name, df in dataframes.items():\n",
    "    print(f\"Updated DataFrame: {df_name}\")\n",
    "    df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "def write_to_delta(dataframes, bronze_location):\n",
    "    \"\"\"\n",
    "    This function checks if each table exists in Delta format at the specified location. \n",
    "    If the table exists, it appends the data; otherwise, it creates a new Delta table.\n",
    "\n",
    "    Args:\n",
    "        dataframes (dict): Dictionary of DataFrames.\n",
    "        bronze_location (str): The base path where Delta tables will be stored.\n",
    "    \"\"\"\n",
    "    for df_name, df in dataframes.items():\n",
    "        delta_table_path = f\"{bronze_location}\\\\{df_name}\"\n",
    "        # Check if the Delta table exists\n",
    "        if DeltaTable.isDeltaTable(spark, delta_table_path):\n",
    "            print(f\"Table {df_name} already exists, appending data.\")\n",
    "            df.write.mode(\"append\").format(\"delta\").save(delta_table_path)\n",
    "        else:\n",
    "            print(f\"Creating new table for {df_name}.\")\n",
    "            df.write.mode(\"overwrite\").format(\"delta\").save(delta_table_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table location_df_raw already exists, appending data.\n",
      "Table payment_df_raw already exists, appending data.\n",
      "Table paymentmethod_df_raw already exists, appending data.\n",
      "Table paymentstatus_df_raw already exists, appending data.\n",
      "Table request_df_raw already exists, appending data.\n",
      "Table trip_df_raw already exists, appending data.\n",
      "Table user_df_raw already exists, appending data.\n",
      "Table vehicles_df_raw already exists, appending data.\n",
      "Table vehiclemakes_df_raw already exists, appending data.\n"
     ]
    }
   ],
   "source": [
    "write_to_delta(dataframes, bronze_location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating external table: bronze.location\n",
      "bronze.location\n",
      "Creating external table: bronze.payment\n",
      "bronze.payment\n",
      "Creating external table: bronze.paymentmethod\n",
      "bronze.paymentmethod\n",
      "Creating external table: bronze.paymentstatus\n",
      "bronze.paymentstatus\n",
      "Creating external table: bronze.request\n",
      "bronze.request\n",
      "Creating external table: bronze.trip\n",
      "bronze.trip\n",
      "Creating external table: bronze.user\n",
      "bronze.user\n",
      "Creating external table: bronze.vehicles\n",
      "bronze.vehicles\n",
      "Creating external table: bronze.vehiclemakes\n",
      "bronze.vehiclemakes\n"
     ]
    }
   ],
   "source": [
    "# After your write_to_delta function\n",
    "for file in file_names:\n",
    "    \n",
    "    table_name = f\"bronze.{file.split('.csv')[0].lower()}\"\n",
    "    print(f\"Creating external table: {table_name}\")\n",
    "    print(table_name.split()[-1])\n",
    "    # Creating schema and external table in Spark SQL\n",
    "    spark.sql(\"CREATE SCHEMA IF NOT EXISTS bronze\")\n",
    "    # Set the current database to 'bronze'\n",
    "    spark.sql(\"USE bronze\")\n",
    "    delta_table_path = f\"{bronze_location}\\\\{table_name.split('.')[-1]}_df_raw\"\n",
    "    spark.sql(f\"CREATE EXTERNAL TABLE IF NOT EXISTS {table_name} USING DELTA LOCATION r'{delta_table_path}'\")\n",
    "    #spark.sql(f\"SELECT * FROM location_df_raw\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+-----------+\n",
      "|namespace|      tableName|isTemporary|\n",
      "+---------+---------------+-----------+\n",
      "|   bronze|       location|      false|\n",
      "|   bronze|        payment|      false|\n",
      "|   bronze|  paymentmethod|      false|\n",
      "|   bronze|  paymentstatus|      false|\n",
      "|   bronze|        request|      false|\n",
      "|   bronze|test_hive_table|      false|\n",
      "|   bronze|           trip|      false|\n",
      "|   bronze|           user|      false|\n",
      "|   bronze|   vehiclemakes|      false|\n",
      "|   bronze|       vehicles|      false|\n",
      "+---------+---------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show all tables in the 'bronze' schema\n",
    "spark.sql(\"SHOW TABLES IN bronze\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Schema:\n",
      "+------------------+\n",
      "|current_database()|\n",
      "+------------------+\n",
      "|            bronze|\n",
      "+------------------+\n",
      "\n",
      "Tables in bronze schema:\n",
      "+---------+---------------+-----------+\n",
      "|namespace|      tableName|isTemporary|\n",
      "+---------+---------------+-----------+\n",
      "|   bronze|       location|      false|\n",
      "|   bronze|        payment|      false|\n",
      "|   bronze|  paymentmethod|      false|\n",
      "|   bronze|  paymentstatus|      false|\n",
      "|   bronze|        request|      false|\n",
      "|   bronze|test_hive_table|      false|\n",
      "|   bronze|           trip|      false|\n",
      "|   bronze|           user|      false|\n",
      "|   bronze|   vehiclemakes|      false|\n",
      "|   bronze|       vehicles|      false|\n",
      "+---------+---------------+-----------+\n",
      "\n",
      "+----------+------------+-----------+-------------------+--------------------+-----------------------------+\n",
      "|LocationID|   Longitude|   Latitude|   _processing_date|     _input_filename|_input_file_modification_date|\n",
      "+----------+------------+-----------+-------------------+--------------------+-----------------------------+\n",
      "|    568573|-73.77675629|40.64520645|2024-10-10 17:44:46|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "+----------+------------+-----------+-------------------+--------------------+-----------------------------+\n",
      "\n",
      "+---------+---------------+---------------+-------------------+--------------------+-----------------------------+\n",
      "|PaymentID|PaymentMethodID|PaymentStatusID|   _processing_date|     _input_filename|_input_file_modification_date|\n",
      "+---------+---------------+---------------+-------------------+--------------------+-----------------------------+\n",
      "|---------|---------------|---------------|2024-10-10 17:50:51|file:///e:/DEPIfI...|         2024-10-08 09:47:...|\n",
      "+---------+---------------+---------------+-------------------+--------------------+-----------------------------+\n",
      "\n",
      "Describing history for bronze.location:\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Verify the current schema\n",
    "print(\"Current Schema:\")\n",
    "spark.sql(\"SELECT current_database()\").show()\n",
    "\n",
    "# List all tables in the bronze schema\n",
    "print(\"Tables in bronze schema:\")\n",
    "spark.sql(\"SHOW TABLES IN bronze\").show()\n",
    "spark.sql(\"SELECT * FROM bronze.location LIMIT 1\").show()\n",
    "spark.sql(\"SELECT * FROM bronze.payment LIMIT 1\").show()\n",
    "\n",
    "\n",
    "# Describe the history of the Delta table\n",
    "print(\"Describing history for bronze.location:\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+------+--------+------------+--------------------------------------------------------------------+----+--------+---------+-----------+-----------------+-------------+----------------------------------------------------------------------+------------+-----------------------------------+\n",
      "|version|timestamp              |userId|userName|operation   |operationParameters                                                 |job |notebook|clusterId|readVersion|isolationLevel   |isBlindAppend|operationMetrics                                                      |userMetadata|engineInfo                         |\n",
      "+-------+-----------------------+------+--------+------------+--------------------------------------------------------------------+----+--------+---------+-----------+-----------------+-------------+----------------------------------------------------------------------+------------+-----------------------------------+\n",
      "|16     |2024-10-11 08:43:31.155|NULL  |NULL    |VACUUM END  |{status -> COMPLETED}                                               |NULL|NULL    |NULL     |15         |SnapshotIsolation|true         |{numDeletedFiles -> 0, numVacuumedDirectories -> 1}                   |NULL        |Apache-Spark/3.5.3 Delta-Lake/3.2.0|\n",
      "|15     |2024-10-11 08:43:29.239|NULL  |NULL    |VACUUM START|{retentionCheckEnabled -> true, defaultRetentionMillis -> 604800000}|NULL|NULL    |NULL     |14         |SnapshotIsolation|true         |{numFilesToDelete -> 0, sizeOfDataToDelete -> 0}                      |NULL        |Apache-Spark/3.5.3 Delta-Lake/3.2.0|\n",
      "|14     |2024-10-11 08:21:48.951|NULL  |NULL    |WRITE       |{mode -> Append, partitionBy -> []}                                 |NULL|NULL    |NULL     |13         |Serializable     |true         |{numFiles -> 10, numOutputRows -> 1148685, numOutputBytes -> 14604261}|NULL        |Apache-Spark/3.5.3 Delta-Lake/3.2.0|\n",
      "|13     |2024-10-10 17:50:50.964|NULL  |NULL    |WRITE       |{mode -> Append, partitionBy -> []}                                 |NULL|NULL    |NULL     |12         |Serializable     |true         |{numFiles -> 10, numOutputRows -> 1148685, numOutputBytes -> 14604261}|NULL        |Apache-Spark/3.5.3 Delta-Lake/3.2.0|\n",
      "|12     |2024-10-10 17:46:25.376|NULL  |NULL    |WRITE       |{mode -> Append, partitionBy -> []}                                 |NULL|NULL    |NULL     |11         |Serializable     |true         |{numFiles -> 10, numOutputRows -> 1148685, numOutputBytes -> 14604251}|NULL        |Apache-Spark/3.5.3 Delta-Lake/3.2.0|\n",
      "|11     |2024-10-10 17:44:47.838|NULL  |NULL    |WRITE       |{mode -> Append, partitionBy -> []}                                 |NULL|NULL    |NULL     |10         |Serializable     |true         |{numFiles -> 10, numOutputRows -> 1148685, numOutputBytes -> 14604261}|NULL        |Apache-Spark/3.5.3 Delta-Lake/3.2.0|\n",
      "|10     |2024-10-10 17:40:48.279|NULL  |NULL    |WRITE       |{mode -> Append, partitionBy -> []}                                 |NULL|NULL    |NULL     |9          |Serializable     |true         |{numFiles -> 10, numOutputRows -> 1148685, numOutputBytes -> 14604261}|NULL        |Apache-Spark/3.5.3 Delta-Lake/3.2.0|\n",
      "|9      |2024-10-10 17:37:55.454|NULL  |NULL    |WRITE       |{mode -> Append, partitionBy -> []}                                 |NULL|NULL    |NULL     |8          |Serializable     |true         |{numFiles -> 10, numOutputRows -> 1148685, numOutputBytes -> 14604261}|NULL        |Apache-Spark/3.5.3 Delta-Lake/3.2.0|\n",
      "|8      |2024-10-10 11:10:47.167|NULL  |NULL    |WRITE       |{mode -> Append, partitionBy -> []}                                 |NULL|NULL    |NULL     |7          |Serializable     |true         |{numFiles -> 10, numOutputRows -> 1148685, numOutputBytes -> 14604261}|NULL        |Apache-Spark/3.5.3 Delta-Lake/3.2.0|\n",
      "|7      |2024-10-10 10:40:59.896|NULL  |NULL    |WRITE       |{mode -> Append, partitionBy -> []}                                 |NULL|NULL    |NULL     |6          |Serializable     |true         |{numFiles -> 10, numOutputRows -> 1148685, numOutputBytes -> 14604261}|NULL        |Apache-Spark/3.5.3 Delta-Lake/3.2.0|\n",
      "|6      |2024-10-10 09:52:51.971|NULL  |NULL    |WRITE       |{mode -> Append, partitionBy -> []}                                 |NULL|NULL    |NULL     |5          |Serializable     |true         |{numFiles -> 10, numOutputRows -> 1148685, numOutputBytes -> 14607661}|NULL        |Apache-Spark/3.5.3 Delta-Lake/3.2.0|\n",
      "|5      |2024-10-10 09:01:22.326|NULL  |NULL    |WRITE       |{mode -> Append, partitionBy -> []}                                 |NULL|NULL    |NULL     |4          |Serializable     |true         |{numFiles -> 10, numOutputRows -> 1148685, numOutputBytes -> 14607661}|NULL        |Apache-Spark/3.5.3 Delta-Lake/3.2.0|\n",
      "|4      |2024-10-10 09:00:29.441|NULL  |NULL    |WRITE       |{mode -> Append, partitionBy -> []}                                 |NULL|NULL    |NULL     |3          |Serializable     |true         |{numFiles -> 10, numOutputRows -> 1148685, numOutputBytes -> 14607661}|NULL        |Apache-Spark/3.5.3 Delta-Lake/3.2.0|\n",
      "|3      |2024-10-10 08:59:23.378|NULL  |NULL    |WRITE       |{mode -> Append, partitionBy -> []}                                 |NULL|NULL    |NULL     |2          |Serializable     |true         |{numFiles -> 10, numOutputRows -> 1148685, numOutputBytes -> 14607661}|NULL        |Apache-Spark/3.5.3 Delta-Lake/3.2.0|\n",
      "|2      |2024-10-10 08:49:23.136|NULL  |NULL    |WRITE       |{mode -> Append, partitionBy -> []}                                 |NULL|NULL    |NULL     |1          |Serializable     |true         |{numFiles -> 10, numOutputRows -> 1148685, numOutputBytes -> 14607661}|NULL        |Apache-Spark/3.5.3 Delta-Lake/3.2.0|\n",
      "|1      |2024-10-10 08:47:23.124|NULL  |NULL    |WRITE       |{mode -> Append, partitionBy -> []}                                 |NULL|NULL    |NULL     |0          |Serializable     |true         |{numFiles -> 10, numOutputRows -> 1148685, numOutputBytes -> 14607661}|NULL        |Apache-Spark/3.5.3 Delta-Lake/3.2.0|\n",
      "|0      |2024-10-10 08:46:59.229|NULL  |NULL    |WRITE       |{mode -> Overwrite, partitionBy -> []}                              |NULL|NULL    |NULL     |NULL       |Serializable     |false        |{numFiles -> 10, numOutputRows -> 1148685, numOutputBytes -> 14607661}|NULL        |Apache-Spark/3.5.3 Delta-Lake/3.2.0|\n",
      "+-------+-----------------------+------+--------+------------+--------------------------------------------------------------------+----+--------+---------+-----------+-----------------+-------------+----------------------------------------------------------------------+------------+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from delta.tables import *\n",
    "\n",
    "deltaTable = DeltaTable.forPath(spark, \"bronze\\\\location_df_raw\")\n",
    "fullHistoryDF = deltaTable.history()\n",
    "fullHistoryDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT _processing_date, count(*) cnt FROM bronze.location GROUP BY _processing_date\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT _processing_date, count(*) cnt FROM bronze.location GROUP BY _processing_date\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----------+-------------------+--------------------+-----------------------------+\n",
      "|LocationID|   Longitude|   Latitude|   _processing_date|     _input_filename|_input_file_modification_date|\n",
      "+----------+------------+-----------+-------------------+--------------------+-----------------------------+\n",
      "|    568573|-73.77675629|40.64520645|2024-10-10 17:44:46|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|    108784|-73.99476624|40.73990250|2024-10-10 17:44:46|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|    507969|-73.95380402|40.76547623|2024-10-10 17:44:46|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|    565124|-73.78221893|40.64458084|2024-10-10 17:44:46|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|    467139|-73.96097565|40.77510834|2024-10-10 17:44:46|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|    314617|-73.97971344|40.76609421|2024-10-10 17:44:46|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|     55692|-74.00254822|40.73046875|2024-10-10 17:44:46|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|    326825|-73.97868347|40.75005722|2024-10-10 17:44:46|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|     98546|-73.99604797|40.73210526|2024-10-10 17:44:46|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|    538464|-73.94026947|40.75105667|2024-10-10 17:44:46|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|     97981|-73.99613190|40.74356079|2024-10-10 17:44:46|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|    441529|-73.96571350|40.76302719|2024-10-10 17:44:46|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|    374871|-73.97437286|40.75043869|2024-10-10 17:44:46|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|    201932|-73.98780823|40.73832703|2024-10-10 17:44:46|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|    322265|-73.97904205|40.75609589|2024-10-10 17:44:46|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|    177390|-73.98955536|40.76268387|2024-10-10 17:44:46|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|    197274|-73.98812866|40.75400925|2024-10-10 17:44:46|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|    177126|-73.98957825|40.77419662|2024-10-10 17:44:46|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|    449973|-73.96398926|40.77089310|2024-10-10 17:44:46|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "|    316012|-73.97959137|40.75189972|2024-10-10 17:44:46|file:///e:/DEPIfI...|         2024-10-08 09:46:...|\n",
      "+----------+------------+-----------+-------------------+--------------------+-----------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from bronze.location\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = spark.read.table(\"bronze.user\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "location\n",
      "bronze.location\n",
      "payment\n",
      "bronze.payment\n",
      "paymentmethod\n",
      "bronze.paymentmethod\n",
      "paymentstatus\n",
      "bronze.paymentstatus\n",
      "request\n",
      "bronze.request\n",
      "trip\n",
      "bronze.trip\n",
      "user\n",
      "bronze.user\n",
      "vehicles\n",
      "bronze.vehicles\n",
      "vehiclemakes\n",
      "bronze.vehiclemakes\n"
     ]
    }
   ],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Dictionary to store the DeltaTable objects\n",
    "delta_tables = {}\n",
    "\n",
    "for file in file_names:\n",
    "    file_name = file.split('.csv')[0].lower()  # Extract the file name (without .csv extension)\n",
    "    table_name = f\"bronze.{file_name}\"  # Define the table name in the bronze schema\n",
    "    \n",
    "    print(file_name)\n",
    "    print(table_name)\n",
    "\n",
    "    # Store the DeltaTable object in the dictionary using the file name as the key\n",
    "    delta_tables[file_name] = DeltaTable.forName(spark, table_name)\n",
    "\n",
    "    # Vacuum the table if it has not been vacuumed in the last 30 days.\n",
    "    if delta_tables[file_name].history(30).filter(\"operation = 'VACUUM START'\").count() == 0:\n",
    "        delta_tables[file_name].optimize()\n",
    "        delta_tables[file_name].vacuum()  # Default = 7 days\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o37.sql.\n: org.sparkproject.guava.util.concurrent.UncheckedExecutionException: java.lang.NullPointerException\r\n\tat org.sparkproject.guava.cache.LocalCache$Segment.get(LocalCache.java:2263)\r\n\tat org.sparkproject.guava.cache.LocalCache.get(LocalCache.java:4000)\r\n\tat org.sparkproject.guava.cache.LocalCache$LocalManualCache.get(LocalCache.java:4789)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.getCachedPlan(SessionCatalog.scala:210)\r\n\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable.org$apache$spark$sql$execution$datasources$FindDataSourceTable$$readDataSourceTable(DataSourceStrategy.scala:248)\r\n\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable$$anonfun$apply$2.applyOrElse(DataSourceStrategy.scala:296)\r\n\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable$$anonfun$apply$2.applyOrElse(DataSourceStrategy.scala:278)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)\r\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\r\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.SubqueryAlias.mapChildren(basicLogicalOperators.scala:1676)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)\r\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\r\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.Aggregate.mapChildren(basicLogicalOperators.scala:1151)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable.apply(DataSourceStrategy.scala:278)\r\n\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable.apply(DataSourceStrategy.scala:241)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\r\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\r\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\r\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\r\n\tat scala.collection.immutable.List.foreach(List.scala:431)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)\r\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\r\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\r\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\r\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\r\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.lang.NullPointerException\r\n\tat org.apache.spark.sql.delta.storage.DelegatingLogStore.<init>(DelegatingLogStore.scala:38)\r\n\tat org.apache.spark.sql.delta.storage.LogStore$.createLogStoreWithClassName(LogStore.scala:288)\r\n\tat org.apache.spark.sql.delta.storage.LogStoreProvider.createLogStore(LogStore.scala:385)\r\n\tat org.apache.spark.sql.delta.storage.LogStoreProvider.createLogStore$(LogStore.scala:380)\r\n\tat org.apache.spark.sql.delta.storage.LogStore$.createLogStore(LogStore.scala:266)\r\n\tat org.apache.spark.sql.delta.storage.LogStore$.apply(LogStore.scala:279)\r\n\tat org.apache.spark.sql.delta.storage.LogStore$.apply(LogStore.scala:274)\r\n\tat org.apache.spark.sql.delta.storage.LogStoreProvider.createLogStore(LogStore.scala:322)\r\n\tat org.apache.spark.sql.delta.storage.LogStoreProvider.createLogStore$(LogStore.scala:321)\r\n\tat org.apache.spark.sql.delta.DeltaLog.createLogStore(DeltaLog.scala:74)\r\n\tat org.apache.spark.sql.delta.DeltaLog.store$lzycompute(DeltaLog.scala:122)\r\n\tat org.apache.spark.sql.delta.DeltaLog.store(DeltaLog.scala:122)\r\n\tat org.apache.spark.sql.delta.Checkpoints.findLastCompleteCheckpointBefore(Checkpoints.scala:441)\r\n\tat org.apache.spark.sql.delta.Checkpoints.findLastCompleteCheckpointBefore$(Checkpoints.scala:431)\r\n\tat org.apache.spark.sql.delta.DeltaLog.findLastCompleteCheckpointBefore(DeltaLog.scala:74)\r\n\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:398)\r\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:168)\r\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:166)\r\n\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:74)\r\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:136)\r\n\tat com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:128)\r\n\tat com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:117)\r\n\tat org.apache.spark.sql.delta.DeltaLog.recordOperation(DeltaLog.scala:74)\r\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:135)\r\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:125)\r\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:115)\r\n\tat org.apache.spark.sql.delta.DeltaLog.recordDeltaOperation(DeltaLog.scala:74)\r\n\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:375)\r\n\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:386)\r\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:168)\r\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:166)\r\n\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:74)\r\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:136)\r\n\tat com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:128)\r\n\tat com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:117)\r\n\tat org.apache.spark.sql.delta.DeltaLog.recordOperation(DeltaLog.scala:74)\r\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:135)\r\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:125)\r\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:115)\r\n\tat org.apache.spark.sql.delta.DeltaLog.recordDeltaOperation(DeltaLog.scala:74)\r\n\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:375)\r\n\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:386)\r\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:168)\r\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:166)\r\n\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:74)\r\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:136)\r\n\tat com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:128)\r\n\tat com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:117)\r\n\tat org.apache.spark.sql.delta.DeltaLog.recordOperation(DeltaLog.scala:74)\r\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:135)\r\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:125)\r\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:115)\r\n\tat org.apache.spark.sql.delta.DeltaLog.recordDeltaOperation(DeltaLog.scala:74)\r\n\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:375)\r\n\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:386)\r\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:168)\r\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:166)\r\n\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:74)\r\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:136)\r\n\tat com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:128)\r\n\tat com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:117)\r\n\tat org.apache.spark.sql.delta.DeltaLog.recordOperation(DeltaLog.scala:74)\r\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:135)\r\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:125)\r\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:115)\r\n\tat org.apache.spark.sql.delta.DeltaLog.recordDeltaOperation(DeltaLog.scala:74)\r\n\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:375)\r\n\tat org.apache.spark.sql.delta.Checkpoints.readLastCheckpointFile(Checkpoints.scala:369)\r\n\tat org.apache.spark.sql.delta.Checkpoints.readLastCheckpointFile$(Checkpoints.scala:368)\r\n\tat org.apache.spark.sql.delta.DeltaLog.readLastCheckpointFile(DeltaLog.scala:74)\r\n\tat org.apache.spark.sql.delta.SnapshotManagement.$anonfun$getSnapshotAtInit$2(SnapshotManagement.scala:575)\r\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:168)\r\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:166)\r\n\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:74)\r\n\tat org.apache.spark.sql.delta.SnapshotManagement.$anonfun$getSnapshotAtInit$1(SnapshotManagement.scala:573)\r\n\tat org.apache.spark.sql.delta.SnapshotManagement.withSnapshotLockInterruptibly(SnapshotManagement.scala:78)\r\n\tat org.apache.spark.sql.delta.SnapshotManagement.withSnapshotLockInterruptibly$(SnapshotManagement.scala:75)\r\n\tat org.apache.spark.sql.delta.DeltaLog.withSnapshotLockInterruptibly(DeltaLog.scala:74)\r\n\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit(SnapshotManagement.scala:573)\r\n\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit$(SnapshotManagement.scala:572)\r\n\tat org.apache.spark.sql.delta.DeltaLog.getSnapshotAtInit(DeltaLog.scala:74)\r\n\tat org.apache.spark.sql.delta.SnapshotManagement.$init$(SnapshotManagement.scala:69)\r\n\tat org.apache.spark.sql.delta.DeltaLog.<init>(DeltaLog.scala:80)\r\n\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$4(DeltaLog.scala:853)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\r\n\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$3(DeltaLog.scala:848)\r\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:168)\r\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:166)\r\n\tat org.apache.spark.sql.delta.DeltaLog$.recordFrameProfile(DeltaLog.scala:651)\r\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:136)\r\n\tat com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:128)\r\n\tat com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:117)\r\n\tat org.apache.spark.sql.delta.DeltaLog$.recordOperation(DeltaLog.scala:651)\r\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:135)\r\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:125)\r\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:115)\r\n\tat org.apache.spark.sql.delta.DeltaLog$.recordDeltaOperation(DeltaLog.scala:651)\r\n\tat org.apache.spark.sql.delta.DeltaLog$.createDeltaLog$1(DeltaLog.scala:847)\r\n\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$5(DeltaLog.scala:866)\r\n\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4792)\r\n\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)\r\n\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)\r\n\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)\r\n\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2257)\r\n\tat com.google.common.cache.LocalCache.get(LocalCache.java:4000)\r\n\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4789)\r\n\tat org.apache.spark.sql.delta.DeltaLog$.getDeltaLogFromCache$1(DeltaLog.scala:865)\r\n\tat org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:875)\r\n\tat org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:751)\r\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:92)\r\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:367)\r\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:92)\r\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:90)\r\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.$anonfun$initialSnapshot$4(DeltaTableV2.scala:145)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:145)\r\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:367)\r\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:144)\r\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:124)\r\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.toBaseRelation$lzycompute(DeltaTableV2.scala:236)\r\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.toBaseRelation(DeltaTableV2.scala:234)\r\n\tat org.apache.spark.sql.delta.sources.DeltaDataSource.$anonfun$createRelation$5(DeltaDataSource.scala:250)\r\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:168)\r\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:166)\r\n\tat org.apache.spark.sql.delta.sources.DeltaDataSource.recordFrameProfile(DeltaDataSource.scala:49)\r\n\tat org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:209)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\r\n\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable.$anonfun$readDataSourceTable$1(DataSourceStrategy.scala:260)\r\n\tat org.sparkproject.guava.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4792)\r\n\tat org.sparkproject.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)\r\n\tat org.sparkproject.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)\r\n\tat org.sparkproject.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)\r\n\tat org.sparkproject.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)\r\n\t... 87 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSELECT COUNT(*) FROM bronze.location\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32mc:\\Users\\mouda\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\sql\\session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[1;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[0;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[0;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[0;32m   1630\u001b[0m         )\n\u001b[1;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\mouda\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\mouda\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\mouda\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o37.sql.\n: org.sparkproject.guava.util.concurrent.UncheckedExecutionException: java.lang.NullPointerException\r\n\tat org.sparkproject.guava.cache.LocalCache$Segment.get(LocalCache.java:2263)\r\n\tat org.sparkproject.guava.cache.LocalCache.get(LocalCache.java:4000)\r\n\tat org.sparkproject.guava.cache.LocalCache$LocalManualCache.get(LocalCache.java:4789)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.getCachedPlan(SessionCatalog.scala:210)\r\n\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable.org$apache$spark$sql$execution$datasources$FindDataSourceTable$$readDataSourceTable(DataSourceStrategy.scala:248)\r\n\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable$$anonfun$apply$2.applyOrElse(DataSourceStrategy.scala:296)\r\n\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable$$anonfun$apply$2.applyOrElse(DataSourceStrategy.scala:278)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)\r\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\r\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.SubqueryAlias.mapChildren(basicLogicalOperators.scala:1676)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)\r\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\r\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.Aggregate.mapChildren(basicLogicalOperators.scala:1151)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable.apply(DataSourceStrategy.scala:278)\r\n\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable.apply(DataSourceStrategy.scala:241)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\r\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\r\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\r\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\r\n\tat scala.collection.immutable.List.foreach(List.scala:431)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)\r\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\r\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\r\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\r\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\r\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.lang.NullPointerException\r\n\tat org.apache.spark.sql.delta.storage.DelegatingLogStore.<init>(DelegatingLogStore.scala:38)\r\n\tat org.apache.spark.sql.delta.storage.LogStore$.createLogStoreWithClassName(LogStore.scala:288)\r\n\tat org.apache.spark.sql.delta.storage.LogStoreProvider.createLogStore(LogStore.scala:385)\r\n\tat org.apache.spark.sql.delta.storage.LogStoreProvider.createLogStore$(LogStore.scala:380)\r\n\tat org.apache.spark.sql.delta.storage.LogStore$.createLogStore(LogStore.scala:266)\r\n\tat org.apache.spark.sql.delta.storage.LogStore$.apply(LogStore.scala:279)\r\n\tat org.apache.spark.sql.delta.storage.LogStore$.apply(LogStore.scala:274)\r\n\tat org.apache.spark.sql.delta.storage.LogStoreProvider.createLogStore(LogStore.scala:322)\r\n\tat org.apache.spark.sql.delta.storage.LogStoreProvider.createLogStore$(LogStore.scala:321)\r\n\tat org.apache.spark.sql.delta.DeltaLog.createLogStore(DeltaLog.scala:74)\r\n\tat org.apache.spark.sql.delta.DeltaLog.store$lzycompute(DeltaLog.scala:122)\r\n\tat org.apache.spark.sql.delta.DeltaLog.store(DeltaLog.scala:122)\r\n\tat org.apache.spark.sql.delta.Checkpoints.findLastCompleteCheckpointBefore(Checkpoints.scala:441)\r\n\tat org.apache.spark.sql.delta.Checkpoints.findLastCompleteCheckpointBefore$(Checkpoints.scala:431)\r\n\tat org.apache.spark.sql.delta.DeltaLog.findLastCompleteCheckpointBefore(DeltaLog.scala:74)\r\n\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:398)\r\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:168)\r\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:166)\r\n\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:74)\r\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:136)\r\n\tat com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:128)\r\n\tat com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:117)\r\n\tat org.apache.spark.sql.delta.DeltaLog.recordOperation(DeltaLog.scala:74)\r\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:135)\r\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:125)\r\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:115)\r\n\tat org.apache.spark.sql.delta.DeltaLog.recordDeltaOperation(DeltaLog.scala:74)\r\n\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:375)\r\n\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:386)\r\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:168)\r\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:166)\r\n\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:74)\r\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:136)\r\n\tat com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:128)\r\n\tat com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:117)\r\n\tat org.apache.spark.sql.delta.DeltaLog.recordOperation(DeltaLog.scala:74)\r\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:135)\r\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:125)\r\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:115)\r\n\tat org.apache.spark.sql.delta.DeltaLog.recordDeltaOperation(DeltaLog.scala:74)\r\n\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:375)\r\n\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:386)\r\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:168)\r\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:166)\r\n\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:74)\r\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:136)\r\n\tat com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:128)\r\n\tat com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:117)\r\n\tat org.apache.spark.sql.delta.DeltaLog.recordOperation(DeltaLog.scala:74)\r\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:135)\r\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:125)\r\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:115)\r\n\tat org.apache.spark.sql.delta.DeltaLog.recordDeltaOperation(DeltaLog.scala:74)\r\n\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:375)\r\n\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:386)\r\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:168)\r\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:166)\r\n\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:74)\r\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:136)\r\n\tat com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:128)\r\n\tat com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:117)\r\n\tat org.apache.spark.sql.delta.DeltaLog.recordOperation(DeltaLog.scala:74)\r\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:135)\r\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:125)\r\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:115)\r\n\tat org.apache.spark.sql.delta.DeltaLog.recordDeltaOperation(DeltaLog.scala:74)\r\n\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:375)\r\n\tat org.apache.spark.sql.delta.Checkpoints.readLastCheckpointFile(Checkpoints.scala:369)\r\n\tat org.apache.spark.sql.delta.Checkpoints.readLastCheckpointFile$(Checkpoints.scala:368)\r\n\tat org.apache.spark.sql.delta.DeltaLog.readLastCheckpointFile(DeltaLog.scala:74)\r\n\tat org.apache.spark.sql.delta.SnapshotManagement.$anonfun$getSnapshotAtInit$2(SnapshotManagement.scala:575)\r\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:168)\r\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:166)\r\n\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:74)\r\n\tat org.apache.spark.sql.delta.SnapshotManagement.$anonfun$getSnapshotAtInit$1(SnapshotManagement.scala:573)\r\n\tat org.apache.spark.sql.delta.SnapshotManagement.withSnapshotLockInterruptibly(SnapshotManagement.scala:78)\r\n\tat org.apache.spark.sql.delta.SnapshotManagement.withSnapshotLockInterruptibly$(SnapshotManagement.scala:75)\r\n\tat org.apache.spark.sql.delta.DeltaLog.withSnapshotLockInterruptibly(DeltaLog.scala:74)\r\n\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit(SnapshotManagement.scala:573)\r\n\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit$(SnapshotManagement.scala:572)\r\n\tat org.apache.spark.sql.delta.DeltaLog.getSnapshotAtInit(DeltaLog.scala:74)\r\n\tat org.apache.spark.sql.delta.SnapshotManagement.$init$(SnapshotManagement.scala:69)\r\n\tat org.apache.spark.sql.delta.DeltaLog.<init>(DeltaLog.scala:80)\r\n\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$4(DeltaLog.scala:853)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\r\n\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$3(DeltaLog.scala:848)\r\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:168)\r\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:166)\r\n\tat org.apache.spark.sql.delta.DeltaLog$.recordFrameProfile(DeltaLog.scala:651)\r\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:136)\r\n\tat com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:128)\r\n\tat com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:117)\r\n\tat org.apache.spark.sql.delta.DeltaLog$.recordOperation(DeltaLog.scala:651)\r\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:135)\r\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:125)\r\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:115)\r\n\tat org.apache.spark.sql.delta.DeltaLog$.recordDeltaOperation(DeltaLog.scala:651)\r\n\tat org.apache.spark.sql.delta.DeltaLog$.createDeltaLog$1(DeltaLog.scala:847)\r\n\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$5(DeltaLog.scala:866)\r\n\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4792)\r\n\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)\r\n\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)\r\n\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)\r\n\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2257)\r\n\tat com.google.common.cache.LocalCache.get(LocalCache.java:4000)\r\n\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4789)\r\n\tat org.apache.spark.sql.delta.DeltaLog$.getDeltaLogFromCache$1(DeltaLog.scala:865)\r\n\tat org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:875)\r\n\tat org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:751)\r\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:92)\r\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:367)\r\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:92)\r\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:90)\r\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.$anonfun$initialSnapshot$4(DeltaTableV2.scala:145)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:145)\r\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:367)\r\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:144)\r\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:124)\r\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.toBaseRelation$lzycompute(DeltaTableV2.scala:236)\r\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.toBaseRelation(DeltaTableV2.scala:234)\r\n\tat org.apache.spark.sql.delta.sources.DeltaDataSource.$anonfun$createRelation$5(DeltaDataSource.scala:250)\r\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:168)\r\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:166)\r\n\tat org.apache.spark.sql.delta.sources.DeltaDataSource.recordFrameProfile(DeltaDataSource.scala:49)\r\n\tat org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:209)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\r\n\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable.$anonfun$readDataSourceTable$1(DataSourceStrategy.scala:260)\r\n\tat org.sparkproject.guava.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4792)\r\n\tat org.sparkproject.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)\r\n\tat org.sparkproject.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)\r\n\tat org.sparkproject.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)\r\n\tat org.sparkproject.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)\r\n\t... 87 more\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT COUNT(*) FROM bronze.location\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
